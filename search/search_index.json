{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the EESSI project documentation! \u00b6 Quote What if there was a way to avoid having to install a broad range of scientific software from scratch on every HPC cluster or cloud instance you use or maintain, without compromising on performance? The European Environment for Scientific Software Installations (EESSI, pronounced as \"easy\") is a collaboration between different European partners in HPC community. The goal of this project is to build a common stack of scientific software installations for HPC systems and beyond, including laptops, personal workstations and cloud infrastructure. More details about the project are available in the different subsections: Project overview Filesystem layer Compatibility layer Software layer Pilot repository Software testing Meetings Project partners Contact info The EESSI project was presented at the 6th EasyBuild User Meeting in January 2021, check the recording:","title":"Home"},{"location":"#welcome-to-the-eessi-project-documentation","text":"Quote What if there was a way to avoid having to install a broad range of scientific software from scratch on every HPC cluster or cloud instance you use or maintain, without compromising on performance? The European Environment for Scientific Software Installations (EESSI, pronounced as \"easy\") is a collaboration between different European partners in HPC community. The goal of this project is to build a common stack of scientific software installations for HPC systems and beyond, including laptops, personal workstations and cloud infrastructure. More details about the project are available in the different subsections: Project overview Filesystem layer Compatibility layer Software layer Pilot repository Software testing Meetings Project partners Contact info The EESSI project was presented at the 6th EasyBuild User Meeting in January 2021, check the recording:","title":"Welcome to the EESSI project documentation!"},{"location":"compatibility_layer/","text":"Compatibility layer \u00b6 The middle layer of the EESSI project is the compatibility layer , which ensures that our scientific software stack is compatible with different client operating systems (different Linux distributions, macOS and even Windows via WSL ). For this we rely on Gentoo Prefix , by installing a limited set of Gentoo Linux packages in a non-standard location (a \"prefix\"), using Gentoo's package manager Portage . The compatible layer is maintained via our https://github.com/EESSI/compatibility-layer GitHub repository.","title":"Compatibility layer"},{"location":"compatibility_layer/#compatibility-layer","text":"The middle layer of the EESSI project is the compatibility layer , which ensures that our scientific software stack is compatible with different client operating systems (different Linux distributions, macOS and even Windows via WSL ). For this we rely on Gentoo Prefix , by installing a limited set of Gentoo Linux packages in a non-standard location (a \"prefix\"), using Gentoo's package manager Portage . The compatible layer is maintained via our https://github.com/EESSI/compatibility-layer GitHub repository.","title":"Compatibility layer"},{"location":"contact/","text":"Contact info \u00b6 For more information: visit our website https://www.eessi-hpc.org consult our documentation at https://eessi.github.io reach out to one of the project partners check out our GitHub repositories at https://github.com/EESSI follow us on Twitter: https://twitter.com/eessi_hpc A Slack channel is available for the EESSI community (an invitation is required to join).","title":"Contact info"},{"location":"contact/#contact-info","text":"For more information: visit our website https://www.eessi-hpc.org consult our documentation at https://eessi.github.io reach out to one of the project partners check out our GitHub repositories at https://github.com/EESSI follow us on Twitter: https://twitter.com/eessi_hpc A Slack channel is available for the EESSI community (an invitation is required to join).","title":"Contact info"},{"location":"filesystem_layer/","text":"Filesystem layer \u00b6 The bottom layer of the EESSI project is the filesystem layer , which is responsible for distributing the software stack. For this we rely on CernVM-FS (or CVMFS for short), a network file system used to distribute the software to the clients in a fast, reliable and scalable way. CVMFS was created over 10 years ago specifically for the purpose of globally distributing a large software stack. For the experiments at the Large Hadron Collider, it hosts several hundred million files and directories that are distributed to the order of hundred thousand client computers. The hierarchical structure with multiple caching layers (Stratum-0, Stratum-1's located at partner sites and local caching proxies) ensures good performance with limited resources. Redundancy is provided by using multiple Stratum-1's at various sites. Since CVMFS is based on the HTTP protocol, the ubiquitous Squid caching proxy can be leveraged to reduce server loads and improve performance at large installations (such as HPC clusters). Clients can easily mount the file system (read-only) via a FUSE (Filesystem in Userspace) module. For a (basic) introduction to CernVM-FS, see this presentation . Detailed information about how we configure CVMFS is available at https://github.com/EESSI/filesystem-layer .","title":"Overview"},{"location":"filesystem_layer/#filesystem-layer","text":"The bottom layer of the EESSI project is the filesystem layer , which is responsible for distributing the software stack. For this we rely on CernVM-FS (or CVMFS for short), a network file system used to distribute the software to the clients in a fast, reliable and scalable way. CVMFS was created over 10 years ago specifically for the purpose of globally distributing a large software stack. For the experiments at the Large Hadron Collider, it hosts several hundred million files and directories that are distributed to the order of hundred thousand client computers. The hierarchical structure with multiple caching layers (Stratum-0, Stratum-1's located at partner sites and local caching proxies) ensures good performance with limited resources. Redundancy is provided by using multiple Stratum-1's at various sites. Since CVMFS is based on the HTTP protocol, the ubiquitous Squid caching proxy can be leveraged to reduce server loads and improve performance at large installations (such as HPC clusters). Clients can easily mount the file system (read-only) via a FUSE (Filesystem in Userspace) module. For a (basic) introduction to CernVM-FS, see this presentation . Detailed information about how we configure CVMFS is available at https://github.com/EESSI/filesystem-layer .","title":"Filesystem layer"},{"location":"meetings/","text":"Meetings \u00b6 Monthly meetings (online) \u00b6 Online EESSI update meeting, every 1st Thursday of the month at 14:00 CE(S)T. More info via https://github.com/EESSI/meetings/wiki Physical meetings \u00b6 EESSI Community Meeting in Amsterdam (NL), 14-16 Sept 2022 Physical meetings (archive) \u00b6 2019 \u00b6 Meeting in Cambridge (UK), 20-21 May 2019 2020 \u00b6 Meeting in Groningen (NL), 16 Jan 2020 Meeting in Delft (NL), 5 Mar 2020","title":"Overview"},{"location":"meetings/#meetings","text":"","title":"Meetings"},{"location":"meetings/#monthly-meetings-online","text":"Online EESSI update meeting, every 1st Thursday of the month at 14:00 CE(S)T. More info via https://github.com/EESSI/meetings/wiki","title":"Monthly meetings (online)"},{"location":"meetings/#physical-meetings","text":"EESSI Community Meeting in Amsterdam (NL), 14-16 Sept 2022","title":"Physical meetings"},{"location":"meetings/#physical-meetings-archive","text":"","title":"Physical meetings (archive)"},{"location":"meetings/#2019","text":"Meeting in Cambridge (UK), 20-21 May 2019","title":"2019"},{"location":"meetings/#2020","text":"Meeting in Groningen (NL), 16 Jan 2020 Meeting in Delft (NL), 5 Mar 2020","title":"2020"},{"location":"overview/","text":"Overview of the EESSI project \u00b6 Scope & Goals \u00b6 Through the EESSI project, we want to set up a shared stack of scientific software installations , and by doing so avoid a lot of duplicate work across HPC sites. For end users, we want to provide a uniform user experience with respect to available scientific software, regardless of which system they use. Our software stack should work on laptops, personal workstations, HPC clusters and in the cloud, which means we will need to support different CPUs, networks, GPUs, and so on. We hope to make this work for any Linux distribution and maybe even macOS and Windows via WSL , and a wide variety of CPU architectures (Intel, AMD, ARM, POWER, RISC-V). Of course we want to focus on the performance of the software, but also on automating the workflow for maintaining the software stack, thoroughly testing the installations, and collaborating efficiently. Inspiration \u00b6 The EESSI concept is heavily inspired by Compute Canada software stack, which is a shared software stack used on all 5 major national systems in Canada and a bunch of smaller ones. The design of the Compute Canada software stack is discussed in detail in the PEARC'19 paper \"Providing a Unified Software Environment for Canada\u2019s National Advanced Computing Centers\" . It has also been presented at the 5th EasyBuild User Meetings ( slides , recorded talk ), and is well documented . Layered structure \u00b6 The EESSI project consists of 3 layers. The bottom layer is the filesystem layer , which is responsible for distributing the software stack across clients. The middle layer is a compatibility layer , which ensures that the software stack is compatible with multiple different client operating systems. The top layer is the software layer , which contains the actual scientific software applications and their dependencies. The host OS still provides a couple of things, like drivers for network and GPU, support for shared filesystems like GPFS and Lustre, a resource manager like Slurm, and so on. Opportunities \u00b6 We hope to collaborate with interested parties across the HPC community, including HPC centres, vendors, consultancy companies and scientific software developers. Through our software stack, HPC users can seamlessly hop between sites, since the same software is available everywhere. We can leverage each others work with respect to providing tested and properly optimized scientific software installations more efficiently, and provide a platform for easy benchmarking of new systems. By working together with the developers of scientific software we can provide vetted installations for the broad HPC community. Challenges \u00b6 There are many challenges in an ambitious project like this, including (but probably not limited to): Finding time and manpower to get the software stack set up properly; Leveraging system sources like network interconnect (MPI & co), accelerators (GPUs), ...; Supporting CPU architectures other than x86_64, including ARM, POWER, RISC-V, ... Dealing with licensed software, like Intel tools, MATLAB, ANSYS, ...; Integration with resource managers (Slurm) and vendor provided software (Cray PE); Convincing HPC site admins to adopt EESSI; Current status \u00b6 (June 2020) We are actively working on a pilot setup that has a limited scope, and are organizing monthly meetings to discuss progress and next steps forward. Keep an eye on our GitHub repositories at https://github.com/EESSI and our Twitter feed .","title":"Project overview"},{"location":"overview/#overview-of-the-eessi-project","text":"","title":"Overview of the EESSI project"},{"location":"overview/#scope-goals","text":"Through the EESSI project, we want to set up a shared stack of scientific software installations , and by doing so avoid a lot of duplicate work across HPC sites. For end users, we want to provide a uniform user experience with respect to available scientific software, regardless of which system they use. Our software stack should work on laptops, personal workstations, HPC clusters and in the cloud, which means we will need to support different CPUs, networks, GPUs, and so on. We hope to make this work for any Linux distribution and maybe even macOS and Windows via WSL , and a wide variety of CPU architectures (Intel, AMD, ARM, POWER, RISC-V). Of course we want to focus on the performance of the software, but also on automating the workflow for maintaining the software stack, thoroughly testing the installations, and collaborating efficiently.","title":"Scope &amp; Goals"},{"location":"overview/#inspiration","text":"The EESSI concept is heavily inspired by Compute Canada software stack, which is a shared software stack used on all 5 major national systems in Canada and a bunch of smaller ones. The design of the Compute Canada software stack is discussed in detail in the PEARC'19 paper \"Providing a Unified Software Environment for Canada\u2019s National Advanced Computing Centers\" . It has also been presented at the 5th EasyBuild User Meetings ( slides , recorded talk ), and is well documented .","title":"Inspiration"},{"location":"overview/#layered-structure","text":"The EESSI project consists of 3 layers. The bottom layer is the filesystem layer , which is responsible for distributing the software stack across clients. The middle layer is a compatibility layer , which ensures that the software stack is compatible with multiple different client operating systems. The top layer is the software layer , which contains the actual scientific software applications and their dependencies. The host OS still provides a couple of things, like drivers for network and GPU, support for shared filesystems like GPFS and Lustre, a resource manager like Slurm, and so on.","title":"Layered structure"},{"location":"overview/#opportunities","text":"We hope to collaborate with interested parties across the HPC community, including HPC centres, vendors, consultancy companies and scientific software developers. Through our software stack, HPC users can seamlessly hop between sites, since the same software is available everywhere. We can leverage each others work with respect to providing tested and properly optimized scientific software installations more efficiently, and provide a platform for easy benchmarking of new systems. By working together with the developers of scientific software we can provide vetted installations for the broad HPC community.","title":"Opportunities"},{"location":"overview/#challenges","text":"There are many challenges in an ambitious project like this, including (but probably not limited to): Finding time and manpower to get the software stack set up properly; Leveraging system sources like network interconnect (MPI & co), accelerators (GPUs), ...; Supporting CPU architectures other than x86_64, including ARM, POWER, RISC-V, ... Dealing with licensed software, like Intel tools, MATLAB, ANSYS, ...; Integration with resource managers (Slurm) and vendor provided software (Cray PE); Convincing HPC site admins to adopt EESSI;","title":"Challenges"},{"location":"overview/#current-status","text":"(June 2020) We are actively working on a pilot setup that has a limited scope, and are organizing monthly meetings to discuss progress and next steps forward. Keep an eye on our GitHub repositories at https://github.com/EESSI and our Twitter feed .","title":"Current status"},{"location":"partners/","text":"Project partners \u00b6 Delft University of Technology (The Netherlands) \u00b6 Robbert Eggermont Koen Mulderij Dell Technologies (Europe) \u00b6 Walther Blom, High Education & Research Jaco van Dijk, Higher Education Eindhoven University of Technology \u00b6 Patrick Van Brakel Ghent University (Belgium) \u00b6 Kenneth Hoste, HPC-UGent HPCNow! (Spain) \u00b6 Oriol Mula Valls J\u00fclich Supercomputing Centre (Germany) \u00b6 Alan O'Cais University of Cambridge (United Kingdom) \u00b6 Mark Sharpley, Research Computing Services Division University of Groningen (The Netherlands) \u00b6 Bob Dr\u00f6ge, Center for Information Technology Henk-Jan Zilverberg, Center for Information Technology University of Twente (The Netherlands) \u00b6 Geert Jan Laanstra, Electrical Engineering, Mathematics and Computer Science (EEMCS) University of Oslo (Norway) \u00b6 Terje Kvernes University of Bergen (Norway) \u00b6 Thomas R\u00f6blitz Vrije Universiteit Amsterdam (The Netherlands) \u00b6 Peter Stol SURF (The Netherlands) \u00b6 Caspar van Leeuwen Marco Verdicchio Bas van der Vlies","title":"Project partners"},{"location":"partners/#project-partners","text":"","title":"Project partners"},{"location":"partners/#delft-university-of-technology-the-netherlands","text":"Robbert Eggermont Koen Mulderij","title":"Delft University of Technology (The Netherlands)"},{"location":"partners/#dell-technologies-europe","text":"Walther Blom, High Education & Research Jaco van Dijk, Higher Education","title":"Dell Technologies (Europe)"},{"location":"partners/#eindhoven-university-of-technology","text":"Patrick Van Brakel","title":"Eindhoven University of Technology"},{"location":"partners/#ghent-university-belgium","text":"Kenneth Hoste, HPC-UGent","title":"Ghent University (Belgium)"},{"location":"partners/#hpcnow-spain","text":"Oriol Mula Valls","title":"HPCNow! (Spain)"},{"location":"partners/#julich-supercomputing-centre-germany","text":"Alan O'Cais","title":"J\u00fclich Supercomputing Centre (Germany)"},{"location":"partners/#university-of-cambridge-united-kingdom","text":"Mark Sharpley, Research Computing Services Division","title":"University of Cambridge (United Kingdom)"},{"location":"partners/#university-of-groningen-the-netherlands","text":"Bob Dr\u00f6ge, Center for Information Technology Henk-Jan Zilverberg, Center for Information Technology","title":"University of Groningen (The Netherlands)"},{"location":"partners/#university-of-twente-the-netherlands","text":"Geert Jan Laanstra, Electrical Engineering, Mathematics and Computer Science (EEMCS)","title":"University of Twente (The Netherlands)"},{"location":"partners/#university-of-oslo-norway","text":"Terje Kvernes","title":"University of Oslo (Norway)"},{"location":"partners/#university-of-bergen-norway","text":"Thomas R\u00f6blitz","title":"University of Bergen (Norway)"},{"location":"partners/#vrije-universiteit-amsterdam-the-netherlands","text":"Peter Stol","title":"Vrije Universiteit Amsterdam (The Netherlands)"},{"location":"partners/#surf-the-netherlands","text":"Caspar van Leeuwen Marco Verdicchio Bas van der Vlies","title":"SURF (The Netherlands)"},{"location":"pilot/","text":"Pilot software stack (2021.12) \u00b6 Caveats \u00b6 The current EESSI pilot software stack (version 2021.12) is the 7th iteration, and there are some known issues and limitations, please take these into account: First of all: the EESSI pilot software stack is NOT READY FOR PRODUCTION! Do not use it for production work, and be careful when testing it on production systems! Reporting problems \u00b6 If you notice any problems, please report them via https://github.com/EESSI/software-layer/issues. Accessing the EESSI pilot repository through Singularity \u00b6 The easiest way to access the EESSI pilot repository is by using Singularity. If Singularity is installed already, no admin privileges are required. No other software is needed either on the host. A container image is available in the GitHub Container Registry (see https://github.com/EESSI/filesystem-layer/pkgs/container/client-pilot ). It only contains a minimal operating system + the necessary packages to access the EESSI pilot repository through CernVM-FS, and it is suitable for aarch64 , ppc64le , and x86_64 . The container image can be used directly by Singularity (no prior download required), as follows: First, create some local directories in /tmp/$USER which will be bind mounted in the container: mkdir -p /tmp/ $USER / { var-lib-cvmfs,var-run-cvmfs,home } These provides space for the CernVM-FS cache, and an empty home directory to use in the container. Set the $SINGULARITY_BIND and $SINGULARITY_HOME environment variables to configure Singularity: export SINGULARITY_BIND = \"/tmp/ $USER /var-run-cvmfs:/var/run/cvmfs,/tmp/ $USER /var-lib-cvmfs:/var/lib/cvmfs\" export SINGULARITY_HOME = \"/tmp/ $USER /home:/home/ $USER \" Start the container using singularity shell , using --fusemount to mount the EESSI pilot repository (using the cvmfs2 command that is included in the container image): export EESSI_PILOT = \"container:cvmfs2 pilot.eessi-hpc.org /cvmfs/pilot.eessi-hpc.org\" singularity shell --fusemount \" $EESSI_PILOT \" docker://ghcr.io/eessi/client-pilot:centos7 This should give you a shell in the container, where the EESSI pilot repository is mounted: $ singularity shell --fusemount \"$EESSI_PILOT\" docker://ghcr.io/eessi/client-pilot:centos7 INFO: Using cached SIF image CernVM-FS: pre-mounted on file descriptor 3 CernVM-FS: loading Fuse module... done Singularity> It is possible that you see some scary looking warnings, but those can be ignored for now. To verify that things are working, check the contents of the /cvmfs/pilot.eessi-hpc.org/versions/2021.12 directory: Singularity> ls /cvmfs/pilot.eessi-hpc.org/versions/2021.12 compat init software Standard installation \u00b6 For those with privileges on their system, there are a number of example installation scripts for different architectures and operating systems available in the EESSI demo repository . Here we prefer the Singularity approach as we can guarantee that the container image is up to date. Setting up the EESSI environment \u00b6 Once you have the EESSI pilot repository mounted, you can set up the environment by sourcing the provided init script: source /cvmfs/pilot.eessi-hpc.org/versions/2021.12/init/bash If all goes well, you should see output like this: Found EESSI pilot repo @ /cvmfs/pilot.eessi-hpc.org/versions/2021.12! Using x86_64/intel/haswell as software subdirectory. Using /cvmfs/pilot.eessi-hpc.org/versions/2021.12/software/linux/x86_64/intel/haswell/modules/all as the directory to be added to MODULEPATH. Found Lmod configuration file at /cvmfs/pilot.eessi-hpc.org/versions/2021.12/software/linux/x86_64/intel/haswell/.lmod/lmodrc.lua Initializing Lmod... Prepending /cvmfs/pilot.eessi-hpc.org/versions/2021.12/software/linux/x86_64/intel/haswell/modules/all to $MODULEPATH ... Environment set up to use EESSI pilot software stack, have fun! [ EESSI pilot 2021 .12 ] $ Now you're all set up! Go ahead and explore the software stack using \" module avail \", and go wild with testing the available software installations! Testing the EESSI pilot software stack \u00b6 Please test the EESSI pilot software stack as you see fit: running simple commands, performing small calculations or running small benchmarks, etc. Test scripts that have been verified to work correctly using the pilot software stack are available at https://github.com/EESSI/software-layer/tree/main/tests . Giving feedback or reporting problems \u00b6 Any feedback is welcome, and questions or problems reports are welcome as well, through one of the EESSI communication channels: ( preferred! ) EESSI software-layer GitHub repository: https://github.com/EESSI/software-layer/issues EESSI mailing list ( eessi@list.rug.nl ) EESSI Slack: https://eessi-hpc.slack.com (get an invite via https://www.eessi-hpc.org/join ) monthly EESSI meetings (first Thursday of the month at 2pm CEST) Available software \u00b6 (last update: Mar 21st 2022) EESSI currently supports the following HPC applications as well as all their dependencies: GROMACS (2020.1 and 2020.4) OpenFOAM (v2006 and 8) R (4.0.0) + R-bundle-Bioconductor (3.11) + RStudio Server (1.3.1093) TensorFlow (2.3.1) and Horovod (0.21.3) OSU-Micro-Benchmarks (5.6.3) ReFrame (3.9.1) Spark (3.1.1) IPython (7.15.0) QuantumESPRESSO (6.6) (currently not available on ppc64le ) WRF (3.9.1.1) [EESSI pilot 2021.12] $ module --nx avail --------------------------- /cvmfs/pilot.eessi-hpc.org/versions/2021.12/software/linux/x86_64/intel/haswell/modules/all ---------------------------- ant/1.10.8-Java-11 LMDB/0.9.24-GCCcore-9.3.0 Arrow/0.17.1-foss-2020a-Python-3.8.2 lz4/1.9.2-GCCcore-9.3.0 Bazel/3.6.0-GCCcore-9.3.0 Mako/1.1.2-GCCcore-9.3.0 Bison/3.5.3-GCCcore-9.3.0 MariaDB-connector-c/3.1.7-GCCcore-9.3.0 Boost/1.72.0-gompi-2020a matplotlib/3.2.1-foss-2020a-Python-3.8.2 cairo/1.16.0-GCCcore-9.3.0 Mesa/20.0.2-GCCcore-9.3.0 CGAL/4.14.3-gompi-2020a-Python-3.8.2 Meson/0.55.1-GCCcore-9.3.0-Python-3.8.2 CMake/3.16.4-GCCcore-9.3.0 METIS/5.1.0-GCCcore-9.3.0 CMake/3.20.1-GCCcore-10.3.0 MPFR/4.0.2-GCCcore-9.3.0 code-server/3.7.3 NASM/2.14.02-GCCcore-9.3.0 DB/18.1.32-GCCcore-9.3.0 ncdf4/1.17-foss-2020a-R-4.0.0 DB/18.1.40-GCCcore-10.3.0 netCDF-Fortran/4.5.2-gompi-2020a double-conversion/3.1.5-GCCcore-9.3.0 netCDF/4.7.4-gompi-2020a Doxygen/1.8.17-GCCcore-9.3.0 nettle/3.6-GCCcore-9.3.0 EasyBuild/4.5.0 networkx/2.4-foss-2020a-Python-3.8.2 EasyBuild/4.5.1 (D) Ninja/1.10.0-GCCcore-9.3.0 Eigen/3.3.7-GCCcore-9.3.0 NLopt/2.6.1-GCCcore-9.3.0 Eigen/3.3.9-GCCcore-10.3.0 NSPR/4.25-GCCcore-9.3.0 ELPA/2019.11.001-foss-2020a NSS/3.51-GCCcore-9.3.0 expat/2.2.9-GCCcore-9.3.0 nsync/1.24.0-GCCcore-9.3.0 expat/2.2.9-GCCcore-10.3.0 numactl/2.0.13-GCCcore-9.3.0 FFmpeg/4.2.2-GCCcore-9.3.0 numactl/2.0.14-GCCcore-10.3.0 FFTW/3.3.8-gompi-2020a OpenBLAS/0.3.9-GCC-9.3.0 FFTW/3.3.9-gompi-2021a OpenBLAS/0.3.15-GCC-10.3.0 flatbuffers/1.12.0-GCCcore-9.3.0 OpenFOAM/v2006-foss-2020a FlexiBLAS/3.0.4-GCC-10.3.0 OpenFOAM/8-foss-2020a (D) fontconfig/2.13.92-GCCcore-9.3.0 OpenMPI/4.0.3-GCC-9.3.0 foss/2020a OpenMPI/4.1.1-GCC-10.3.0 foss/2021a OpenPGM/5.2.122-GCCcore-9.3.0 freetype/2.10.1-GCCcore-9.3.0 OpenSSL/1.1 (D) FriBidi/1.0.9-GCCcore-9.3.0 OSU-Micro-Benchmarks/5.6.3-gompi-2020a GCC/9.3.0 Pango/1.44.7-GCCcore-9.3.0 GCC/10.3.0 ParaView/5.8.0-foss-2020a-Python-3.8.2-mpi GCCcore/9.3.0 PCRE/8.44-GCCcore-9.3.0 GCCcore/10.3.0 PCRE2/10.34-GCCcore-9.3.0 Ghostscript/9.52-GCCcore-9.3.0 Perl/5.30.2-GCCcore-9.3.0 giflib/5.2.1-GCCcore-9.3.0 Perl/5.32.1-GCCcore-10.3.0 git/2.23.0-GCCcore-9.3.0-nodocs pixman/0.38.4-GCCcore-9.3.0 git/2.32.0-GCCcore-10.3.0-nodocs (D) pkg-config/0.29.2-GCCcore-9.3.0 GLib/2.64.1-GCCcore-9.3.0 pkg-config/0.29.2-GCCcore-10.3.0 GLPK/4.65-GCCcore-9.3.0 pkg-config/0.29.2 (D) GMP/6.2.0-GCCcore-9.3.0 pkgconfig/1.5.1-GCCcore-9.3.0-Python-3.8.2 GMP/6.2.1-GCCcore-10.3.0 PMIx/3.1.5-GCCcore-9.3.0 gnuplot/5.2.8-GCCcore-9.3.0 PMIx/3.2.3-GCCcore-10.3.0 GObject-Introspection/1.64.0-GCCcore-9.3.0-Python-3.8.2 poetry/1.0.9-GCCcore-9.3.0-Python-3.8.2 gompi/2020a protobuf-python/3.13.0-foss-2020a-Python-3.8.2 gompi/2021a protobuf/3.13.0-GCCcore-9.3.0 groff/1.22.4-GCCcore-9.3.0 pybind11/2.4.3-GCCcore-9.3.0-Python-3.8.2 groff/1.22.4-GCCcore-10.3.0 pybind11/2.6.2-GCCcore-10.3.0 GROMACS/2020.1-foss-2020a-Python-3.8.2 Python/2.7.18-GCCcore-9.3.0 GROMACS/2020.4-foss-2020a-Python-3.8.2 (D) Python/3.8.2-GCCcore-9.3.0 GSL/2.6-GCC-9.3.0 Python/3.9.5-GCCcore-10.3.0-bare gzip/1.10-GCCcore-9.3.0 Python/3.9.5-GCCcore-10.3.0 h5py/2.10.0-foss-2020a-Python-3.8.2 PyYAML/5.3-GCCcore-9.3.0 HarfBuzz/2.6.4-GCCcore-9.3.0 Qt5/5.14.1-GCCcore-9.3.0 HDF5/1.10.6-gompi-2020a QuantumESPRESSO/6.6-foss-2020a Horovod/0.21.3-foss-2020a-TensorFlow-2.3.1-Python-3.8.2 R-bundle-Bioconductor/3.11-foss-2020a-R-4.0.0 hwloc/2.2.0-GCCcore-9.3.0 R/4.0.0-foss-2020a hwloc/2.4.1-GCCcore-10.3.0 re2c/1.3-GCCcore-9.3.0 hypothesis/6.13.1-GCCcore-10.3.0 RStudio-Server/1.3.1093-foss-2020a-Java-11-R-4.0.0 ICU/66.1-GCCcore-9.3.0 Rust/1.52.1-GCCcore-10.3.0 ImageMagick/7.0.10-1-GCCcore-9.3.0 ScaLAPACK/2.1.0-gompi-2020a IPython/7.15.0-foss-2020a-Python-3.8.2 ScaLAPACK/2.1.0-gompi-2021a-fb JasPer/2.0.14-GCCcore-9.3.0 scikit-build/0.10.0-foss-2020a-Python-3.8.2 Java/11.0.2 (11) SciPy-bundle/2020.03-foss-2020a-Python-3.8.2 jbigkit/2.1-GCCcore-9.3.0 SciPy-bundle/2021.05-foss-2021a JsonCpp/1.9.4-GCCcore-9.3.0 SCOTCH/6.0.9-gompi-2020a LAME/3.100-GCCcore-9.3.0 snappy/1.1.8-GCCcore-9.3.0 libarchive/3.5.1-GCCcore-10.3.0 Spark/3.1.1-foss-2020a-Python-3.8.2 libcerf/1.13-GCCcore-9.3.0 SQLite/3.31.1-GCCcore-9.3.0 libdrm/2.4.100-GCCcore-9.3.0 SQLite/3.35.4-GCCcore-10.3.0 libevent/2.1.11-GCCcore-9.3.0 SWIG/4.0.1-GCCcore-9.3.0 libevent/2.1.12-GCCcore-10.3.0 Szip/2.1.1-GCCcore-9.3.0 libfabric/1.11.0-GCCcore-9.3.0 Tcl/8.6.10-GCCcore-9.3.0 libfabric/1.12.1-GCCcore-10.3.0 Tcl/8.6.11-GCCcore-10.3.0 libffi/3.3-GCCcore-9.3.0 tcsh/6.22.02-GCCcore-9.3.0 libffi/3.3-GCCcore-10.3.0 TensorFlow/2.3.1-foss-2020a-Python-3.8.2 libgd/2.3.0-GCCcore-9.3.0 time/1.9-GCCcore-9.3.0 libGLU/9.0.1-GCCcore-9.3.0 Tk/8.6.10-GCCcore-9.3.0 libglvnd/1.2.0-GCCcore-9.3.0 Tkinter/3.8.2-GCCcore-9.3.0 libiconv/1.16-GCCcore-9.3.0 UCX/1.8.0-GCCcore-9.3.0 libjpeg-turbo/2.0.4-GCCcore-9.3.0 UCX/1.10.0-GCCcore-10.3.0 libpciaccess/0.16-GCCcore-9.3.0 UDUNITS/2.2.26-foss-2020a libpciaccess/0.16-GCCcore-10.3.0 UnZip/6.0-GCCcore-9.3.0 libpng/1.6.37-GCCcore-9.3.0 UnZip/6.0-GCCcore-10.3.0 libsndfile/1.0.28-GCCcore-9.3.0 WRF/3.9.1.1-foss-2020a-dmpar libsodium/1.0.18-GCCcore-9.3.0 X11/20200222-GCCcore-9.3.0 LibTIFF/4.1.0-GCCcore-9.3.0 x264/20191217-GCCcore-9.3.0 libtirpc/1.2.6-GCCcore-9.3.0 x265/3.3-GCCcore-9.3.0 libunwind/1.3.1-GCCcore-9.3.0 xorg-macros/1.19.2-GCCcore-9.3.0 libxc/4.3.4-GCC-9.3.0 xorg-macros/1.19.3-GCCcore-10.3.0 libxml2/2.9.10-GCCcore-9.3.0 Xvfb/1.20.9-GCCcore-9.3.0 libxml2/2.9.10-GCCcore-10.3.0 Yasm/1.3.0-GCCcore-9.3.0 libyaml/0.2.2-GCCcore-9.3.0 ZeroMQ/4.3.2-GCCcore-9.3.0 LittleCMS/2.9-GCCcore-9.3.0 Zip/3.0-GCCcore-9.3.0 LLVM/9.0.1-GCCcore-9.3.0 zstd/1.4.4-GCCcore-9.3.0 Architecture and micro-architecture support \u00b6 x86_64 \u00b6 generic (currently implies march=x86-64 and -mtune=generic ) AMD zen2 (Rome) zen3 (Milan) Intel haswell skylake_avx512 aarch64/arm64 \u00b6 generic (currently implies -march=armv8-a and -mtune=generic ) AWS Graviton2 ppc64le \u00b6 generic power9le EasyBuild configuration \u00b6 EasyBuild v4.5.1 was used to install the software in the 2021.12 version of the pilot repository. For some installations pull requests with changes that will be included in later EasyBuild versions were leveraged, see the build script that was used. An example configuration of the build environment based on https://github.com/EESSI/software-layer can be seen here: $ eb --show-config # # Current EasyBuild configuration # (C: command line argument, D: default value, E: environment variable, F: configuration file) # buildpath (E) = /tmp/eessi-build/easybuild/build containerpath (E) = /tmp/eessi-build/easybuild/containers debug (E) = True filter-deps (E) = Autoconf, Automake, Autotools, binutils, bzip2, cURL, DBus, flex, gettext, gperf, help2man, intltool, libreadline, libtool, Lua, M4, makeinfo, ncurses, util-linux, XZ, zlib filter-env-vars (E) = LD_LIBRARY_PATH hooks (E) = /home/eessi-build/software-layer/eb_hooks.py ignore-osdeps (E) = True installpath (E) = /cvmfs/pilot.eessi-hpc.org/2021.06/software/linux/x86_64/intel/haswell module-extensions (E) = True packagepath (E) = /tmp/eessi-build/easybuild/packages prefix (E) = /tmp/eessi-build/easybuild repositorypath (E) = /tmp/eessi-build/easybuild/ebfiles_repo robot-paths (D) = /cvmfs/pilot.eessi-hpc.org/versions/2021.12/software/linux/x86_64/intel/haswell/software/EasyBuild/4.5.1/easybuild/easyconfigs rpath (E) = True sourcepath (E) = /tmp/eessi-build/easybuild/sources: sysroot (E) = /cvmfs/pilot.eessi-hpc.org/versions/2021.12/compat/linux/x86_64 trace (E) = True zip-logs (E) = bzip2","title":"Pilot repository"},{"location":"pilot/#pilot-software-stack-202112","text":"","title":"Pilot software stack (2021.12)"},{"location":"pilot/#caveats","text":"The current EESSI pilot software stack (version 2021.12) is the 7th iteration, and there are some known issues and limitations, please take these into account: First of all: the EESSI pilot software stack is NOT READY FOR PRODUCTION! Do not use it for production work, and be careful when testing it on production systems!","title":"Caveats"},{"location":"pilot/#reporting-problems","text":"If you notice any problems, please report them via https://github.com/EESSI/software-layer/issues.","title":"Reporting problems"},{"location":"pilot/#accessing-the-eessi-pilot-repository-through-singularity","text":"The easiest way to access the EESSI pilot repository is by using Singularity. If Singularity is installed already, no admin privileges are required. No other software is needed either on the host. A container image is available in the GitHub Container Registry (see https://github.com/EESSI/filesystem-layer/pkgs/container/client-pilot ). It only contains a minimal operating system + the necessary packages to access the EESSI pilot repository through CernVM-FS, and it is suitable for aarch64 , ppc64le , and x86_64 . The container image can be used directly by Singularity (no prior download required), as follows: First, create some local directories in /tmp/$USER which will be bind mounted in the container: mkdir -p /tmp/ $USER / { var-lib-cvmfs,var-run-cvmfs,home } These provides space for the CernVM-FS cache, and an empty home directory to use in the container. Set the $SINGULARITY_BIND and $SINGULARITY_HOME environment variables to configure Singularity: export SINGULARITY_BIND = \"/tmp/ $USER /var-run-cvmfs:/var/run/cvmfs,/tmp/ $USER /var-lib-cvmfs:/var/lib/cvmfs\" export SINGULARITY_HOME = \"/tmp/ $USER /home:/home/ $USER \" Start the container using singularity shell , using --fusemount to mount the EESSI pilot repository (using the cvmfs2 command that is included in the container image): export EESSI_PILOT = \"container:cvmfs2 pilot.eessi-hpc.org /cvmfs/pilot.eessi-hpc.org\" singularity shell --fusemount \" $EESSI_PILOT \" docker://ghcr.io/eessi/client-pilot:centos7 This should give you a shell in the container, where the EESSI pilot repository is mounted: $ singularity shell --fusemount \"$EESSI_PILOT\" docker://ghcr.io/eessi/client-pilot:centos7 INFO: Using cached SIF image CernVM-FS: pre-mounted on file descriptor 3 CernVM-FS: loading Fuse module... done Singularity> It is possible that you see some scary looking warnings, but those can be ignored for now. To verify that things are working, check the contents of the /cvmfs/pilot.eessi-hpc.org/versions/2021.12 directory: Singularity> ls /cvmfs/pilot.eessi-hpc.org/versions/2021.12 compat init software","title":"Accessing the EESSI pilot repository through Singularity"},{"location":"pilot/#standard-installation","text":"For those with privileges on their system, there are a number of example installation scripts for different architectures and operating systems available in the EESSI demo repository . Here we prefer the Singularity approach as we can guarantee that the container image is up to date.","title":"Standard installation"},{"location":"pilot/#setting-up-the-eessi-environment","text":"Once you have the EESSI pilot repository mounted, you can set up the environment by sourcing the provided init script: source /cvmfs/pilot.eessi-hpc.org/versions/2021.12/init/bash If all goes well, you should see output like this: Found EESSI pilot repo @ /cvmfs/pilot.eessi-hpc.org/versions/2021.12! Using x86_64/intel/haswell as software subdirectory. Using /cvmfs/pilot.eessi-hpc.org/versions/2021.12/software/linux/x86_64/intel/haswell/modules/all as the directory to be added to MODULEPATH. Found Lmod configuration file at /cvmfs/pilot.eessi-hpc.org/versions/2021.12/software/linux/x86_64/intel/haswell/.lmod/lmodrc.lua Initializing Lmod... Prepending /cvmfs/pilot.eessi-hpc.org/versions/2021.12/software/linux/x86_64/intel/haswell/modules/all to $MODULEPATH ... Environment set up to use EESSI pilot software stack, have fun! [ EESSI pilot 2021 .12 ] $ Now you're all set up! Go ahead and explore the software stack using \" module avail \", and go wild with testing the available software installations!","title":"Setting up the EESSI environment"},{"location":"pilot/#testing-the-eessi-pilot-software-stack","text":"Please test the EESSI pilot software stack as you see fit: running simple commands, performing small calculations or running small benchmarks, etc. Test scripts that have been verified to work correctly using the pilot software stack are available at https://github.com/EESSI/software-layer/tree/main/tests .","title":"Testing the EESSI pilot software stack"},{"location":"pilot/#giving-feedback-or-reporting-problems","text":"Any feedback is welcome, and questions or problems reports are welcome as well, through one of the EESSI communication channels: ( preferred! ) EESSI software-layer GitHub repository: https://github.com/EESSI/software-layer/issues EESSI mailing list ( eessi@list.rug.nl ) EESSI Slack: https://eessi-hpc.slack.com (get an invite via https://www.eessi-hpc.org/join ) monthly EESSI meetings (first Thursday of the month at 2pm CEST)","title":"Giving feedback or reporting problems"},{"location":"pilot/#available-software","text":"(last update: Mar 21st 2022) EESSI currently supports the following HPC applications as well as all their dependencies: GROMACS (2020.1 and 2020.4) OpenFOAM (v2006 and 8) R (4.0.0) + R-bundle-Bioconductor (3.11) + RStudio Server (1.3.1093) TensorFlow (2.3.1) and Horovod (0.21.3) OSU-Micro-Benchmarks (5.6.3) ReFrame (3.9.1) Spark (3.1.1) IPython (7.15.0) QuantumESPRESSO (6.6) (currently not available on ppc64le ) WRF (3.9.1.1) [EESSI pilot 2021.12] $ module --nx avail --------------------------- /cvmfs/pilot.eessi-hpc.org/versions/2021.12/software/linux/x86_64/intel/haswell/modules/all ---------------------------- ant/1.10.8-Java-11 LMDB/0.9.24-GCCcore-9.3.0 Arrow/0.17.1-foss-2020a-Python-3.8.2 lz4/1.9.2-GCCcore-9.3.0 Bazel/3.6.0-GCCcore-9.3.0 Mako/1.1.2-GCCcore-9.3.0 Bison/3.5.3-GCCcore-9.3.0 MariaDB-connector-c/3.1.7-GCCcore-9.3.0 Boost/1.72.0-gompi-2020a matplotlib/3.2.1-foss-2020a-Python-3.8.2 cairo/1.16.0-GCCcore-9.3.0 Mesa/20.0.2-GCCcore-9.3.0 CGAL/4.14.3-gompi-2020a-Python-3.8.2 Meson/0.55.1-GCCcore-9.3.0-Python-3.8.2 CMake/3.16.4-GCCcore-9.3.0 METIS/5.1.0-GCCcore-9.3.0 CMake/3.20.1-GCCcore-10.3.0 MPFR/4.0.2-GCCcore-9.3.0 code-server/3.7.3 NASM/2.14.02-GCCcore-9.3.0 DB/18.1.32-GCCcore-9.3.0 ncdf4/1.17-foss-2020a-R-4.0.0 DB/18.1.40-GCCcore-10.3.0 netCDF-Fortran/4.5.2-gompi-2020a double-conversion/3.1.5-GCCcore-9.3.0 netCDF/4.7.4-gompi-2020a Doxygen/1.8.17-GCCcore-9.3.0 nettle/3.6-GCCcore-9.3.0 EasyBuild/4.5.0 networkx/2.4-foss-2020a-Python-3.8.2 EasyBuild/4.5.1 (D) Ninja/1.10.0-GCCcore-9.3.0 Eigen/3.3.7-GCCcore-9.3.0 NLopt/2.6.1-GCCcore-9.3.0 Eigen/3.3.9-GCCcore-10.3.0 NSPR/4.25-GCCcore-9.3.0 ELPA/2019.11.001-foss-2020a NSS/3.51-GCCcore-9.3.0 expat/2.2.9-GCCcore-9.3.0 nsync/1.24.0-GCCcore-9.3.0 expat/2.2.9-GCCcore-10.3.0 numactl/2.0.13-GCCcore-9.3.0 FFmpeg/4.2.2-GCCcore-9.3.0 numactl/2.0.14-GCCcore-10.3.0 FFTW/3.3.8-gompi-2020a OpenBLAS/0.3.9-GCC-9.3.0 FFTW/3.3.9-gompi-2021a OpenBLAS/0.3.15-GCC-10.3.0 flatbuffers/1.12.0-GCCcore-9.3.0 OpenFOAM/v2006-foss-2020a FlexiBLAS/3.0.4-GCC-10.3.0 OpenFOAM/8-foss-2020a (D) fontconfig/2.13.92-GCCcore-9.3.0 OpenMPI/4.0.3-GCC-9.3.0 foss/2020a OpenMPI/4.1.1-GCC-10.3.0 foss/2021a OpenPGM/5.2.122-GCCcore-9.3.0 freetype/2.10.1-GCCcore-9.3.0 OpenSSL/1.1 (D) FriBidi/1.0.9-GCCcore-9.3.0 OSU-Micro-Benchmarks/5.6.3-gompi-2020a GCC/9.3.0 Pango/1.44.7-GCCcore-9.3.0 GCC/10.3.0 ParaView/5.8.0-foss-2020a-Python-3.8.2-mpi GCCcore/9.3.0 PCRE/8.44-GCCcore-9.3.0 GCCcore/10.3.0 PCRE2/10.34-GCCcore-9.3.0 Ghostscript/9.52-GCCcore-9.3.0 Perl/5.30.2-GCCcore-9.3.0 giflib/5.2.1-GCCcore-9.3.0 Perl/5.32.1-GCCcore-10.3.0 git/2.23.0-GCCcore-9.3.0-nodocs pixman/0.38.4-GCCcore-9.3.0 git/2.32.0-GCCcore-10.3.0-nodocs (D) pkg-config/0.29.2-GCCcore-9.3.0 GLib/2.64.1-GCCcore-9.3.0 pkg-config/0.29.2-GCCcore-10.3.0 GLPK/4.65-GCCcore-9.3.0 pkg-config/0.29.2 (D) GMP/6.2.0-GCCcore-9.3.0 pkgconfig/1.5.1-GCCcore-9.3.0-Python-3.8.2 GMP/6.2.1-GCCcore-10.3.0 PMIx/3.1.5-GCCcore-9.3.0 gnuplot/5.2.8-GCCcore-9.3.0 PMIx/3.2.3-GCCcore-10.3.0 GObject-Introspection/1.64.0-GCCcore-9.3.0-Python-3.8.2 poetry/1.0.9-GCCcore-9.3.0-Python-3.8.2 gompi/2020a protobuf-python/3.13.0-foss-2020a-Python-3.8.2 gompi/2021a protobuf/3.13.0-GCCcore-9.3.0 groff/1.22.4-GCCcore-9.3.0 pybind11/2.4.3-GCCcore-9.3.0-Python-3.8.2 groff/1.22.4-GCCcore-10.3.0 pybind11/2.6.2-GCCcore-10.3.0 GROMACS/2020.1-foss-2020a-Python-3.8.2 Python/2.7.18-GCCcore-9.3.0 GROMACS/2020.4-foss-2020a-Python-3.8.2 (D) Python/3.8.2-GCCcore-9.3.0 GSL/2.6-GCC-9.3.0 Python/3.9.5-GCCcore-10.3.0-bare gzip/1.10-GCCcore-9.3.0 Python/3.9.5-GCCcore-10.3.0 h5py/2.10.0-foss-2020a-Python-3.8.2 PyYAML/5.3-GCCcore-9.3.0 HarfBuzz/2.6.4-GCCcore-9.3.0 Qt5/5.14.1-GCCcore-9.3.0 HDF5/1.10.6-gompi-2020a QuantumESPRESSO/6.6-foss-2020a Horovod/0.21.3-foss-2020a-TensorFlow-2.3.1-Python-3.8.2 R-bundle-Bioconductor/3.11-foss-2020a-R-4.0.0 hwloc/2.2.0-GCCcore-9.3.0 R/4.0.0-foss-2020a hwloc/2.4.1-GCCcore-10.3.0 re2c/1.3-GCCcore-9.3.0 hypothesis/6.13.1-GCCcore-10.3.0 RStudio-Server/1.3.1093-foss-2020a-Java-11-R-4.0.0 ICU/66.1-GCCcore-9.3.0 Rust/1.52.1-GCCcore-10.3.0 ImageMagick/7.0.10-1-GCCcore-9.3.0 ScaLAPACK/2.1.0-gompi-2020a IPython/7.15.0-foss-2020a-Python-3.8.2 ScaLAPACK/2.1.0-gompi-2021a-fb JasPer/2.0.14-GCCcore-9.3.0 scikit-build/0.10.0-foss-2020a-Python-3.8.2 Java/11.0.2 (11) SciPy-bundle/2020.03-foss-2020a-Python-3.8.2 jbigkit/2.1-GCCcore-9.3.0 SciPy-bundle/2021.05-foss-2021a JsonCpp/1.9.4-GCCcore-9.3.0 SCOTCH/6.0.9-gompi-2020a LAME/3.100-GCCcore-9.3.0 snappy/1.1.8-GCCcore-9.3.0 libarchive/3.5.1-GCCcore-10.3.0 Spark/3.1.1-foss-2020a-Python-3.8.2 libcerf/1.13-GCCcore-9.3.0 SQLite/3.31.1-GCCcore-9.3.0 libdrm/2.4.100-GCCcore-9.3.0 SQLite/3.35.4-GCCcore-10.3.0 libevent/2.1.11-GCCcore-9.3.0 SWIG/4.0.1-GCCcore-9.3.0 libevent/2.1.12-GCCcore-10.3.0 Szip/2.1.1-GCCcore-9.3.0 libfabric/1.11.0-GCCcore-9.3.0 Tcl/8.6.10-GCCcore-9.3.0 libfabric/1.12.1-GCCcore-10.3.0 Tcl/8.6.11-GCCcore-10.3.0 libffi/3.3-GCCcore-9.3.0 tcsh/6.22.02-GCCcore-9.3.0 libffi/3.3-GCCcore-10.3.0 TensorFlow/2.3.1-foss-2020a-Python-3.8.2 libgd/2.3.0-GCCcore-9.3.0 time/1.9-GCCcore-9.3.0 libGLU/9.0.1-GCCcore-9.3.0 Tk/8.6.10-GCCcore-9.3.0 libglvnd/1.2.0-GCCcore-9.3.0 Tkinter/3.8.2-GCCcore-9.3.0 libiconv/1.16-GCCcore-9.3.0 UCX/1.8.0-GCCcore-9.3.0 libjpeg-turbo/2.0.4-GCCcore-9.3.0 UCX/1.10.0-GCCcore-10.3.0 libpciaccess/0.16-GCCcore-9.3.0 UDUNITS/2.2.26-foss-2020a libpciaccess/0.16-GCCcore-10.3.0 UnZip/6.0-GCCcore-9.3.0 libpng/1.6.37-GCCcore-9.3.0 UnZip/6.0-GCCcore-10.3.0 libsndfile/1.0.28-GCCcore-9.3.0 WRF/3.9.1.1-foss-2020a-dmpar libsodium/1.0.18-GCCcore-9.3.0 X11/20200222-GCCcore-9.3.0 LibTIFF/4.1.0-GCCcore-9.3.0 x264/20191217-GCCcore-9.3.0 libtirpc/1.2.6-GCCcore-9.3.0 x265/3.3-GCCcore-9.3.0 libunwind/1.3.1-GCCcore-9.3.0 xorg-macros/1.19.2-GCCcore-9.3.0 libxc/4.3.4-GCC-9.3.0 xorg-macros/1.19.3-GCCcore-10.3.0 libxml2/2.9.10-GCCcore-9.3.0 Xvfb/1.20.9-GCCcore-9.3.0 libxml2/2.9.10-GCCcore-10.3.0 Yasm/1.3.0-GCCcore-9.3.0 libyaml/0.2.2-GCCcore-9.3.0 ZeroMQ/4.3.2-GCCcore-9.3.0 LittleCMS/2.9-GCCcore-9.3.0 Zip/3.0-GCCcore-9.3.0 LLVM/9.0.1-GCCcore-9.3.0 zstd/1.4.4-GCCcore-9.3.0","title":"Available software"},{"location":"pilot/#architecture-and-micro-architecture-support","text":"","title":"Architecture and micro-architecture support"},{"location":"pilot/#x86_64","text":"generic (currently implies march=x86-64 and -mtune=generic ) AMD zen2 (Rome) zen3 (Milan) Intel haswell skylake_avx512","title":"x86_64"},{"location":"pilot/#aarch64arm64","text":"generic (currently implies -march=armv8-a and -mtune=generic ) AWS Graviton2","title":"aarch64/arm64"},{"location":"pilot/#ppc64le","text":"generic power9le","title":"ppc64le"},{"location":"pilot/#easybuild-configuration","text":"EasyBuild v4.5.1 was used to install the software in the 2021.12 version of the pilot repository. For some installations pull requests with changes that will be included in later EasyBuild versions were leveraged, see the build script that was used. An example configuration of the build environment based on https://github.com/EESSI/software-layer can be seen here: $ eb --show-config # # Current EasyBuild configuration # (C: command line argument, D: default value, E: environment variable, F: configuration file) # buildpath (E) = /tmp/eessi-build/easybuild/build containerpath (E) = /tmp/eessi-build/easybuild/containers debug (E) = True filter-deps (E) = Autoconf, Automake, Autotools, binutils, bzip2, cURL, DBus, flex, gettext, gperf, help2man, intltool, libreadline, libtool, Lua, M4, makeinfo, ncurses, util-linux, XZ, zlib filter-env-vars (E) = LD_LIBRARY_PATH hooks (E) = /home/eessi-build/software-layer/eb_hooks.py ignore-osdeps (E) = True installpath (E) = /cvmfs/pilot.eessi-hpc.org/2021.06/software/linux/x86_64/intel/haswell module-extensions (E) = True packagepath (E) = /tmp/eessi-build/easybuild/packages prefix (E) = /tmp/eessi-build/easybuild repositorypath (E) = /tmp/eessi-build/easybuild/ebfiles_repo robot-paths (D) = /cvmfs/pilot.eessi-hpc.org/versions/2021.12/software/linux/x86_64/intel/haswell/software/EasyBuild/4.5.1/easybuild/easyconfigs rpath (E) = True sourcepath (E) = /tmp/eessi-build/easybuild/sources: sysroot (E) = /cvmfs/pilot.eessi-hpc.org/versions/2021.12/compat/linux/x86_64 trace (E) = True zip-logs (E) = bzip2","title":"EasyBuild configuration"},{"location":"software_layer/","text":"Software layer \u00b6 The top layer of the EESSI project is the software layer , which provides the actual scientific software installations. To install the software we include in our stack, we use EasyBuild , a framework for installing scientific software on HPC systems. These installations are optimized for a particular system architecture (specific CPU and GPU generation). To access these software installation we provide environment module files and use Lmod , a modern environment modules tool which has been widely adopted in the HPC community in recent years. We leverage the archspec Python library to automatically select the best suited part of the software stack for a particular host, based on its system architecture. The software layer is maintained through our https://github.com/EESSI/software-layer GitHub repository.","title":"Overview"},{"location":"software_layer/#software-layer","text":"The top layer of the EESSI project is the software layer , which provides the actual scientific software installations. To install the software we include in our stack, we use EasyBuild , a framework for installing scientific software on HPC systems. These installations are optimized for a particular system architecture (specific CPU and GPU generation). To access these software installation we provide environment module files and use Lmod , a modern environment modules tool which has been widely adopted in the HPC community in recent years. We leverage the archspec Python library to automatically select the best suited part of the software stack for a particular host, based on its system architecture. The software layer is maintained through our https://github.com/EESSI/software-layer GitHub repository.","title":"Software layer"},{"location":"software_testing/","text":"Software testing \u00b6 WARNING: development of the software test suite has only just started and is a work in progress. This page describes how the test suite will be designed, but many things are not implemented yet and the design may still change. Description of the software test suite \u00b6 Framework \u00b6 The EESSI project uses the ReFrame framework for software testing. ReFrame is designed particularly for testing HPC software and thus has well integrated support for interacting with schedulers, as well as various launchers for MPI programs. Test variants \u00b6 The EESSI software stack can be used in various ways, e.g. by using the container or when the CVMFS software stack is mounted natively. This means the commands that need to be run to test an application are different in both cases. Similarly, systems may have different hardware (CPUs v.s. GPUs, system size, etc). Thus, tests - e.g. a GROMACS test - may have different variants: one designed to run on CPUs, one on GPUs, one designed to run through the container, etc. The main goal of the EESSI test suite is to test the software stack on systems that have the EESSI CVMFS mounted natively. Some tests may also have variants that can run the same test through the container, but note that this setup is technically much more difficult. Thus, the main focus is on tests that run with a native CVMFS mount of the EESSI stack. By default, ReFrame runs all test variants it find. Thus, in our test suite, we prespecify a number of tags that can be used to select an appropriate subset of tests for your system. We recognize the following tags: container: tests that use the EESSI container to run the software. E.g. one variant of our GROMACS test uses singularity exec to launch the EESSI container, load the GROMACS module, and run the GROMACS test. native : tests that rely on the EESSI software stack being available through the modules system. E.g. one variant of the GROMACS test loads the GROMACS module and runs the GROMACS test. singlecore : tests designed to run on a single core singlenode : tests designed to run on a single (multicore) node (note: may still use MPI for multiprocessing) small : tests designed to run on 2-8 nodes. large : tests designed to run on >9 nodes. cpu : test designed to run on CPU. gpu , gpu_nvidia, gpu_amd: test designed to run on GPUs / nvidia GPUs / AMD GPUs. How to run the test suite \u00b6 General requirements \u00b6 A copy of the tests directory from software repository Requirements for container-based tests \u00b6 Specifically for container-based tests, there are some requirements on the host system: An installation of ReFrame An MPI installation (to launch MPI tests) or PMIx-based launcher (e.g. SLURM compiled with PMIx support) Singularity The container based tests will use a so-called shared alien CVMFS cache to store temporary data. In addition, they use a local CVMFS cache for speed. For this reason, the container tests need to be pointed to one directory that is shared between nodes on your system, and one directory that is node-specific (preferably a local disk). The shared_alien_cache_minimal.sh script that is part of the test suite defines these, and sets up the correct CVMFS configuration. You will have to adapt the SHAREDSPACE and LOCALSPACE variables in that script for your system, and point them to a shared and node-local directory. Setting up a ReFrame configuration file \u00b6 Once the prerequisites have been met, you'll need to create a ReFrame configuration file that matches your system (see the ReFrame documentation ). If you want to use the container-based tests, you have to define a partition programming environment called container and make sure it loads any modules needed to provide the MPI installation and singularity command. For an example configuration file, check the tests/reframe/config/settings.py in the software-layer repository . Other than (potential) adaptations to the container environment, you should only really need to change the systems part. Adapting the tests to your system \u00b6 For now, you will have to adapt the number of tasks specified in full-node tests to match the number of cores your machine has in a single node (in the future, you should be able to do this through the reframe configuration file). To do so, change all self.num_tasks_per_node you find in the various tests to that core count (unless they are 1, in which case the test specifically intended for only 1 process per node). An example run \u00b6 In this example, we assume your current directory is the tests/reframe folder. To list e.g. all single node, cpu-based application tests on a system that has the EESSI software environment available natively, you execute: reframe --config-file=config/settings.py --checkpath eessi-checks/applications/ -l -t native -t single -t cpu (assuming you adapted the config file in config/settings.py for your system). This should list the tests that are selected based on the provided tags. To run the tests, change the -l argument into a -r : reframe --config-file=config/settings.py --checkpath eessi-checks/applications/ -l -t native -t single -t cpu --performance-report To run the same tests with using the EESSI container, run: reframe --config-file=config/settings.py --checkpath eessi-checks/applications/ -l -t container -t single -t cpu --performance-report Note that not all tests necessarily have implementations to run using the EESSI container: the primary focus of the test suite is for HPC sites to check the performance of their software suite. Such sites should have CVMFS mounted natively for optimal performance anyway.","title":"Software testing"},{"location":"software_testing/#software-testing","text":"WARNING: development of the software test suite has only just started and is a work in progress. This page describes how the test suite will be designed, but many things are not implemented yet and the design may still change.","title":"Software testing"},{"location":"software_testing/#description-of-the-software-test-suite","text":"","title":"Description of the software test suite"},{"location":"software_testing/#framework","text":"The EESSI project uses the ReFrame framework for software testing. ReFrame is designed particularly for testing HPC software and thus has well integrated support for interacting with schedulers, as well as various launchers for MPI programs.","title":"Framework"},{"location":"software_testing/#test-variants","text":"The EESSI software stack can be used in various ways, e.g. by using the container or when the CVMFS software stack is mounted natively. This means the commands that need to be run to test an application are different in both cases. Similarly, systems may have different hardware (CPUs v.s. GPUs, system size, etc). Thus, tests - e.g. a GROMACS test - may have different variants: one designed to run on CPUs, one on GPUs, one designed to run through the container, etc. The main goal of the EESSI test suite is to test the software stack on systems that have the EESSI CVMFS mounted natively. Some tests may also have variants that can run the same test through the container, but note that this setup is technically much more difficult. Thus, the main focus is on tests that run with a native CVMFS mount of the EESSI stack. By default, ReFrame runs all test variants it find. Thus, in our test suite, we prespecify a number of tags that can be used to select an appropriate subset of tests for your system. We recognize the following tags: container: tests that use the EESSI container to run the software. E.g. one variant of our GROMACS test uses singularity exec to launch the EESSI container, load the GROMACS module, and run the GROMACS test. native : tests that rely on the EESSI software stack being available through the modules system. E.g. one variant of the GROMACS test loads the GROMACS module and runs the GROMACS test. singlecore : tests designed to run on a single core singlenode : tests designed to run on a single (multicore) node (note: may still use MPI for multiprocessing) small : tests designed to run on 2-8 nodes. large : tests designed to run on >9 nodes. cpu : test designed to run on CPU. gpu , gpu_nvidia, gpu_amd: test designed to run on GPUs / nvidia GPUs / AMD GPUs.","title":"Test variants"},{"location":"software_testing/#how-to-run-the-test-suite","text":"","title":"How to run the test suite"},{"location":"software_testing/#general-requirements","text":"A copy of the tests directory from software repository","title":"General requirements"},{"location":"software_testing/#requirements-for-container-based-tests","text":"Specifically for container-based tests, there are some requirements on the host system: An installation of ReFrame An MPI installation (to launch MPI tests) or PMIx-based launcher (e.g. SLURM compiled with PMIx support) Singularity The container based tests will use a so-called shared alien CVMFS cache to store temporary data. In addition, they use a local CVMFS cache for speed. For this reason, the container tests need to be pointed to one directory that is shared between nodes on your system, and one directory that is node-specific (preferably a local disk). The shared_alien_cache_minimal.sh script that is part of the test suite defines these, and sets up the correct CVMFS configuration. You will have to adapt the SHAREDSPACE and LOCALSPACE variables in that script for your system, and point them to a shared and node-local directory.","title":"Requirements for container-based tests"},{"location":"software_testing/#setting-up-a-reframe-configuration-file","text":"Once the prerequisites have been met, you'll need to create a ReFrame configuration file that matches your system (see the ReFrame documentation ). If you want to use the container-based tests, you have to define a partition programming environment called container and make sure it loads any modules needed to provide the MPI installation and singularity command. For an example configuration file, check the tests/reframe/config/settings.py in the software-layer repository . Other than (potential) adaptations to the container environment, you should only really need to change the systems part.","title":"Setting up a ReFrame configuration file"},{"location":"software_testing/#adapting-the-tests-to-your-system","text":"For now, you will have to adapt the number of tasks specified in full-node tests to match the number of cores your machine has in a single node (in the future, you should be able to do this through the reframe configuration file). To do so, change all self.num_tasks_per_node you find in the various tests to that core count (unless they are 1, in which case the test specifically intended for only 1 process per node).","title":"Adapting the tests to your system"},{"location":"software_testing/#an-example-run","text":"In this example, we assume your current directory is the tests/reframe folder. To list e.g. all single node, cpu-based application tests on a system that has the EESSI software environment available natively, you execute: reframe --config-file=config/settings.py --checkpath eessi-checks/applications/ -l -t native -t single -t cpu (assuming you adapted the config file in config/settings.py for your system). This should list the tests that are selected based on the provided tags. To run the tests, change the -l argument into a -r : reframe --config-file=config/settings.py --checkpath eessi-checks/applications/ -l -t native -t single -t cpu --performance-report To run the same tests with using the EESSI container, run: reframe --config-file=config/settings.py --checkpath eessi-checks/applications/ -l -t container -t single -t cpu --performance-report Note that not all tests necessarily have implementations to run using the EESSI container: the primary focus of the test suite is for HPC sites to check the performance of their software suite. Such sites should have CVMFS mounted natively for optimal performance anyway.","title":"An example run"},{"location":"filesystem_layer/stratum1/","text":"Setting up a Stratum 1 \u00b6 Setting up a Stratum 1 involves the following steps: set up the Stratum 1, preferably by running the Ansible playbook that we provide; request a Stratum 0 firewall exception for your Stratum 1 server; request a <your site>.stratum1.cvmfs.eessi-infra.org DNS entry; open a pull request to include the URL to your Stratum 1 in the EESSI configuration. The last two steps can be skipped if you want to host a \"private\" Stratum 1 for your site. Requirements for a Stratum 1 \u00b6 The main requirements for a Stratum 1 server are a good network connection to the clients it is going to serve, and sufficient disk space. For the EESSI pilot, a few hundred gigabytes should suffice, but for production environments at least 1 TB would be recommended. In terms of cores and memory, a machine with just a few (~4) cores and 4-8 GB of memory should suffice. Various Linux distributions are supported, but we recommend one based on RHEL 7 or 8. Finally, make sure that ports 80 (for the Apache web server) and 8000 are open. Step 1: set up the Stratum 1 \u00b6 The recommended way for setting up an EESSI Stratum 1 is by running the Ansible playbook stratum1.yml from the filesystem-layer repository on GitHub . Installing a Stratum 1 requires a GEO API license key, which will be used to find the (geographically) closest Stratum 1 server for your client and proxies. More information on how to (freely) obtain this key is available in the CVMFS documentation: https://cvmfs.readthedocs.io/en/stable/cpt-replica.html#geo-api-setup. You can put your license key in the local configuration file inventory/local_site_specific_vars.yml . Furthermore, the Stratum 1 runs a Squid server. The template configuration file can be found at templates/eessi_stratum1_squid.conf.j2 . If you want to customize it, for instance for limiting the access to the Stratum 1, you can make your own version of this template file and point to it by setting local_stratum1_cvmfs_squid_conf_src in inventory/local_site_specific_vars.yml . See the comments in the example file for more details. Start by installing Ansible: sudo yum install -y ansible Then install Ansible roles for EESSI: ansible-galaxy role install -r requirements.yml -p ./roles --force Make sure you have enough space in /srv (on the Stratum 1) since the snapshot of the Stratum 0 will end up there by default. To alter the directory where the snapshot gets copied to you can add this variable in inventory/host_vars/<url-or-ip-to-your-stratum1> : cvmfs_srv_mount: /srv Make sure that you have added the hostname or IP address of your server to the inventory/hosts file. Finally, install the Stratum 1 using one of the two following options. Option 1: # -b to run as root, optionally use -K if a sudo password is required ansible-playbook -b [ -K ] -e @inventory/local_site_specific_vars.yml stratum1.yml Option2: Create a ssh key pair and make sure the ansible-host-keys.pub is in the $HOME/.ssh/authorized_keys file on your Stratum 1 server. ssh-keygen -b 2048 -t rsa -f ~/.ssh/ansible-host-keys -q -N \"\" Then run the playbook: ansible-playbook -b --private-key ~/.ssh/ansible-host-keys -e @inventory/local_site_specific_vars.yml stratum1.yml Running the playbook will automatically make replicas of all the repositories defined in group_vars/all.yml . Step 2: request a firewall exception \u00b6 (This step is not implemented yet and can be skipped) You can request a firewall exception rule to be added for your Stratum 1 server by opening an issue on the GitHub page of the filesystem layer repository . Make sure to include the IP address of your server. Step 3: Verification of the Stratum 1 \u00b6 When the playbook has finished your Stratum 1 should be ready. In order to test your Stratum 1, even without a client installed, you can use curl . curl --head http://<url-or-ip-to-your-stratum1>/cvmfs/pilot.eessi-hpc.org/.cvmfspublished This should return: HTTP/1.1 200 OK ... X-Cache: MISS from <url-or-ip-to-your-stratum1> The second time you run it, you should get a cache hit: X-Cache: HIT from <url-or-ip-to-your-stratum1> Example with the Norwegian Stratum 1: curl --head http://bgo-no.stratum1.cvmfs.eessi-infra.org/cvmfs/pilot.eessi-hpc.org/.cvmfspublished You can also test access to your Stratum 1 from a client, for which you will have to install the CVMFS client . Then run the following command to add your newly created Stratum 1 to the existing list of EESSI Stratum 1 servers by creating a local CVMFS configuration file: echo 'CVMFS_SERVER_URL=\"http://<url-or-ip-to-your-stratum1>/cvmfs/@fqrn@;$CVMFS_SERVER_URL\"' | sudo tee -a /etc/cvmfs/domain.d/eessi-hpc.org.local If this is the first time you set up the client you now run: sudo cvmfs_config setup If you already had configured the client before, you can simply reload the config: sudo cvmfs_config reload -c pilot.eessi-hpc.org Finally, verify that the client connects to your new Stratum 1 by running: cvmfs_config stat -v pilot.eessi-hpc.org Assuming that your new Stratum 1 is the geographically closest one to your client, this should return: Connection: http://<url-or-ip-to-your-stratum1>/cvmfs/pilot.eessi-hpc.org through proxy DIRECT ( online ) Step 4: request an EESSI DNS name \u00b6 In order to keep the configuration clean and easy, all the EESSI Stratum 1 servers have a DNS name <your site>.stratum1.cvmfs.eessi-infra.org , where <your site> is often a short name or abbreviation followed by the country code (e.g. rug-nl or bgo-no ). You can request this for your Stratum 1 by mentioning this in the issue that you created in Step 2, or by opening another issue. Step 5: include your Stratum 1 in the EESSI configuration \u00b6 If you want to include your Stratum 1 in the EESSI configuration, i.e. allow any (nearby) client to be able to use it, you can open a pull request with updated configuration files. You will only have to add the URL to your Stratum 1 to the urls list of the eessi_cvmfs_server_urls variable in the all.yml file .","title":"Setting up a Stratum 1"},{"location":"filesystem_layer/stratum1/#setting-up-a-stratum-1","text":"Setting up a Stratum 1 involves the following steps: set up the Stratum 1, preferably by running the Ansible playbook that we provide; request a Stratum 0 firewall exception for your Stratum 1 server; request a <your site>.stratum1.cvmfs.eessi-infra.org DNS entry; open a pull request to include the URL to your Stratum 1 in the EESSI configuration. The last two steps can be skipped if you want to host a \"private\" Stratum 1 for your site.","title":"Setting up a Stratum 1"},{"location":"filesystem_layer/stratum1/#requirements-for-a-stratum-1","text":"The main requirements for a Stratum 1 server are a good network connection to the clients it is going to serve, and sufficient disk space. For the EESSI pilot, a few hundred gigabytes should suffice, but for production environments at least 1 TB would be recommended. In terms of cores and memory, a machine with just a few (~4) cores and 4-8 GB of memory should suffice. Various Linux distributions are supported, but we recommend one based on RHEL 7 or 8. Finally, make sure that ports 80 (for the Apache web server) and 8000 are open.","title":"Requirements for a Stratum 1"},{"location":"filesystem_layer/stratum1/#step-1-set-up-the-stratum-1","text":"The recommended way for setting up an EESSI Stratum 1 is by running the Ansible playbook stratum1.yml from the filesystem-layer repository on GitHub . Installing a Stratum 1 requires a GEO API license key, which will be used to find the (geographically) closest Stratum 1 server for your client and proxies. More information on how to (freely) obtain this key is available in the CVMFS documentation: https://cvmfs.readthedocs.io/en/stable/cpt-replica.html#geo-api-setup. You can put your license key in the local configuration file inventory/local_site_specific_vars.yml . Furthermore, the Stratum 1 runs a Squid server. The template configuration file can be found at templates/eessi_stratum1_squid.conf.j2 . If you want to customize it, for instance for limiting the access to the Stratum 1, you can make your own version of this template file and point to it by setting local_stratum1_cvmfs_squid_conf_src in inventory/local_site_specific_vars.yml . See the comments in the example file for more details. Start by installing Ansible: sudo yum install -y ansible Then install Ansible roles for EESSI: ansible-galaxy role install -r requirements.yml -p ./roles --force Make sure you have enough space in /srv (on the Stratum 1) since the snapshot of the Stratum 0 will end up there by default. To alter the directory where the snapshot gets copied to you can add this variable in inventory/host_vars/<url-or-ip-to-your-stratum1> : cvmfs_srv_mount: /srv Make sure that you have added the hostname or IP address of your server to the inventory/hosts file. Finally, install the Stratum 1 using one of the two following options. Option 1: # -b to run as root, optionally use -K if a sudo password is required ansible-playbook -b [ -K ] -e @inventory/local_site_specific_vars.yml stratum1.yml Option2: Create a ssh key pair and make sure the ansible-host-keys.pub is in the $HOME/.ssh/authorized_keys file on your Stratum 1 server. ssh-keygen -b 2048 -t rsa -f ~/.ssh/ansible-host-keys -q -N \"\" Then run the playbook: ansible-playbook -b --private-key ~/.ssh/ansible-host-keys -e @inventory/local_site_specific_vars.yml stratum1.yml Running the playbook will automatically make replicas of all the repositories defined in group_vars/all.yml .","title":"Step 1: set up the Stratum 1"},{"location":"filesystem_layer/stratum1/#step-2-request-a-firewall-exception","text":"(This step is not implemented yet and can be skipped) You can request a firewall exception rule to be added for your Stratum 1 server by opening an issue on the GitHub page of the filesystem layer repository . Make sure to include the IP address of your server.","title":"Step 2: request a firewall exception"},{"location":"filesystem_layer/stratum1/#step-3-verification-of-the-stratum-1","text":"When the playbook has finished your Stratum 1 should be ready. In order to test your Stratum 1, even without a client installed, you can use curl . curl --head http://<url-or-ip-to-your-stratum1>/cvmfs/pilot.eessi-hpc.org/.cvmfspublished This should return: HTTP/1.1 200 OK ... X-Cache: MISS from <url-or-ip-to-your-stratum1> The second time you run it, you should get a cache hit: X-Cache: HIT from <url-or-ip-to-your-stratum1> Example with the Norwegian Stratum 1: curl --head http://bgo-no.stratum1.cvmfs.eessi-infra.org/cvmfs/pilot.eessi-hpc.org/.cvmfspublished You can also test access to your Stratum 1 from a client, for which you will have to install the CVMFS client . Then run the following command to add your newly created Stratum 1 to the existing list of EESSI Stratum 1 servers by creating a local CVMFS configuration file: echo 'CVMFS_SERVER_URL=\"http://<url-or-ip-to-your-stratum1>/cvmfs/@fqrn@;$CVMFS_SERVER_URL\"' | sudo tee -a /etc/cvmfs/domain.d/eessi-hpc.org.local If this is the first time you set up the client you now run: sudo cvmfs_config setup If you already had configured the client before, you can simply reload the config: sudo cvmfs_config reload -c pilot.eessi-hpc.org Finally, verify that the client connects to your new Stratum 1 by running: cvmfs_config stat -v pilot.eessi-hpc.org Assuming that your new Stratum 1 is the geographically closest one to your client, this should return: Connection: http://<url-or-ip-to-your-stratum1>/cvmfs/pilot.eessi-hpc.org through proxy DIRECT ( online )","title":"Step 3: Verification of the Stratum 1"},{"location":"filesystem_layer/stratum1/#step-4-request-an-eessi-dns-name","text":"In order to keep the configuration clean and easy, all the EESSI Stratum 1 servers have a DNS name <your site>.stratum1.cvmfs.eessi-infra.org , where <your site> is often a short name or abbreviation followed by the country code (e.g. rug-nl or bgo-no ). You can request this for your Stratum 1 by mentioning this in the issue that you created in Step 2, or by opening another issue.","title":"Step 4: request an EESSI DNS name"},{"location":"filesystem_layer/stratum1/#step-5-include-your-stratum-1-in-the-eessi-configuration","text":"If you want to include your Stratum 1 in the EESSI configuration, i.e. allow any (nearby) client to be able to use it, you can open a pull request with updated configuration files. You will only have to add the URL to your Stratum 1 to the urls list of the eessi_cvmfs_server_urls variable in the all.yml file .","title":"Step 5: include your Stratum 1 in the EESSI configuration"},{"location":"meetings/2022-09-amsterdam/","text":"EESSI Community Meeting (Sept'22, Amsterdam) \u00b6 Practical info \u00b6 dates: Wed-Fri 14-16 Sept'22 in conjunction with CernVM workshop @ Nikhef (Mon-Tue 12-13 Sept'22) venue: \"Polderzaal\" at Cafe-Restaurant Polder ( Google Maps ), sponsored by SURF registration ( closed since Fri 9 Sept'22 ) Slack channel: community-meeting-2022 in EESSI Slack YouTube playlist with recorded talks Agenda \u00b6 (subject to changes) We envision a mix of presentations, experience reports, demos, and hands-on sessions and/or hackathons related to the EESSI project. If you would like to give a talk or host a session, please let us know via the EESSI Slack! Wed 14 Sept 2022 \u00b6 [10:00-13:00] Welcome session [10:00-10:30] Walk-in, coffee [10:30-12:00] Round table discussion ( not live-streamed! ) [12:00-13:00] Lunch [13:00-15:00] Presentations on EESSI [13:00-13:30] Introduction to EESSI (Caspar) [ slides - recording ] [13:30-14:00] Hands-on: how to use EESSI (Kenneth) [ slides - recording ] [14:00-14:30] EESSI use cases (Kenneth) [ (slides - recording ] [14:30-15:00] EESSI for sysadmins (Thomas) [ slides - recording ] [15:00-15:30] Coffee break [15:30-17:00] Presentations on EESSI (continued) [15:30-16:00] Hands-on: installing EESSI (Thomas/Kenneth) [16:00-16:45] ComputeCanada site talk (Bart Oldeman, remote) [ slides - recording ] [16:45-17:15] Magic Castle (Felix-Antoine Fortin, remote) [ slides - recording ] [19:00-...] Group dinner @ Saravanaa Bhavan (sponsored by Dell Technologies) address: Stadhouderskade 123-124, Amsterdam Thu 15 Sept 2022 \u00b6 [09:30-12:00] More focused presentations on aspects of EESSI [09:30-10:00] EESSI behind the scenes: compat layer (Bob) [ slides - recording ] [10:00-10:30] EESSI behind the scenes: software layer (Kenneth) [ slides - recording ] [10:30-11:00] Coffee break [11:00-11:30] EESSI behind the scenes: infrastructure (Terje) [ slides - recording ] [11:30-12:00] Status on RISC-V support (Kenneth) [ slides - recording ] [12:00-13:00] Lunch [13:00-14:00] Discussions/hands-on sessions/hackathon [14:00-14:30] Status on GPU support (Alan) [ slides - recording ] [14:30-15:00] Status on build-and-deploy bot (Thomas) [ slides - recording ] [15:00-15:30] Coffee break [15:30-17:00] Discussions/hands-on sessions/hackathon (continued) Hands-on with GPUs (Alan) Hands-on with bot (Thomas/Kenneth) [19:00-...] Group dinner @ Italia Oggi (sponsored by HPC-UGent) address: Binnen Bantammerstraat 11, Amsterdam Fri 16 Sept 2022 \u00b6 [09:30-12:00] Presentations on future work [09:30-10:00] Testing in software layer (Caspar) [ slides - recording ] [10:00-10:30] MultiXscale project (Alan) [ slides - recording ] [10:30-11:00] Coffee break [11:00-11:30] Short-term future work (Kenneth) [ slides - recording ] [11:30-12:00] Discussion: future management structure of EESSI (Alan) [ slides - recording ] [12:00-13:00] Lunch [13:00-14:00] Site reports [ recording ] NESSI (Thomas) [ slides ] NLPL (Stephan) [ slides ] HPCNow! (Danilo) [ slides ] Azure (Hugo) [ slides ] [14:00-14:30] Discussion: what would make or break EESSI for your site? ( notes - recording ) [14:30-15:45] Discussions/hands-on sessions/hackathon Hands-on with GPU support (Alan) Hands-on with bot (Thomas/Kenneth) Hands-on with software testing (Caspar) We need to leave the room by 16:00!","title":"Community meeting (Sept'22)"},{"location":"meetings/2022-09-amsterdam/#eessi-community-meeting-sept22-amsterdam","text":"","title":"EESSI Community Meeting (Sept'22, Amsterdam)"},{"location":"meetings/2022-09-amsterdam/#practical-info","text":"dates: Wed-Fri 14-16 Sept'22 in conjunction with CernVM workshop @ Nikhef (Mon-Tue 12-13 Sept'22) venue: \"Polderzaal\" at Cafe-Restaurant Polder ( Google Maps ), sponsored by SURF registration ( closed since Fri 9 Sept'22 ) Slack channel: community-meeting-2022 in EESSI Slack YouTube playlist with recorded talks","title":"Practical info"},{"location":"meetings/2022-09-amsterdam/#agenda","text":"(subject to changes) We envision a mix of presentations, experience reports, demos, and hands-on sessions and/or hackathons related to the EESSI project. If you would like to give a talk or host a session, please let us know via the EESSI Slack!","title":"Agenda"},{"location":"meetings/2022-09-amsterdam/#wed-14-sept-2022","text":"[10:00-13:00] Welcome session [10:00-10:30] Walk-in, coffee [10:30-12:00] Round table discussion ( not live-streamed! ) [12:00-13:00] Lunch [13:00-15:00] Presentations on EESSI [13:00-13:30] Introduction to EESSI (Caspar) [ slides - recording ] [13:30-14:00] Hands-on: how to use EESSI (Kenneth) [ slides - recording ] [14:00-14:30] EESSI use cases (Kenneth) [ (slides - recording ] [14:30-15:00] EESSI for sysadmins (Thomas) [ slides - recording ] [15:00-15:30] Coffee break [15:30-17:00] Presentations on EESSI (continued) [15:30-16:00] Hands-on: installing EESSI (Thomas/Kenneth) [16:00-16:45] ComputeCanada site talk (Bart Oldeman, remote) [ slides - recording ] [16:45-17:15] Magic Castle (Felix-Antoine Fortin, remote) [ slides - recording ] [19:00-...] Group dinner @ Saravanaa Bhavan (sponsored by Dell Technologies) address: Stadhouderskade 123-124, Amsterdam","title":"Wed 14 Sept 2022"},{"location":"meetings/2022-09-amsterdam/#thu-15-sept-2022","text":"[09:30-12:00] More focused presentations on aspects of EESSI [09:30-10:00] EESSI behind the scenes: compat layer (Bob) [ slides - recording ] [10:00-10:30] EESSI behind the scenes: software layer (Kenneth) [ slides - recording ] [10:30-11:00] Coffee break [11:00-11:30] EESSI behind the scenes: infrastructure (Terje) [ slides - recording ] [11:30-12:00] Status on RISC-V support (Kenneth) [ slides - recording ] [12:00-13:00] Lunch [13:00-14:00] Discussions/hands-on sessions/hackathon [14:00-14:30] Status on GPU support (Alan) [ slides - recording ] [14:30-15:00] Status on build-and-deploy bot (Thomas) [ slides - recording ] [15:00-15:30] Coffee break [15:30-17:00] Discussions/hands-on sessions/hackathon (continued) Hands-on with GPUs (Alan) Hands-on with bot (Thomas/Kenneth) [19:00-...] Group dinner @ Italia Oggi (sponsored by HPC-UGent) address: Binnen Bantammerstraat 11, Amsterdam","title":"Thu 15 Sept 2022"},{"location":"meetings/2022-09-amsterdam/#fri-16-sept-2022","text":"[09:30-12:00] Presentations on future work [09:30-10:00] Testing in software layer (Caspar) [ slides - recording ] [10:00-10:30] MultiXscale project (Alan) [ slides - recording ] [10:30-11:00] Coffee break [11:00-11:30] Short-term future work (Kenneth) [ slides - recording ] [11:30-12:00] Discussion: future management structure of EESSI (Alan) [ slides - recording ] [12:00-13:00] Lunch [13:00-14:00] Site reports [ recording ] NESSI (Thomas) [ slides ] NLPL (Stephan) [ slides ] HPCNow! (Danilo) [ slides ] Azure (Hugo) [ slides ] [14:00-14:30] Discussion: what would make or break EESSI for your site? ( notes - recording ) [14:30-15:45] Discussions/hands-on sessions/hackathon Hands-on with GPU support (Alan) Hands-on with bot (Thomas/Kenneth) Hands-on with software testing (Caspar) We need to leave the room by 16:00!","title":"Fri 16 Sept 2022"},{"location":"software_layer/build_nodes/","text":"Build nodes \u00b6 Any system can be used as a build node to create additional software installations that should be added to the EESSI CernVM-FS repository. Requirements \u00b6 OS and software: GNU/Linux (any distribution) as operating system; a recent version of Singularity (>= 3.6 is recommended); check with singularity --version screen or tmux is highly recommended; Admin privileges are not required, as long as Singularity is installed. Resources: 8 or more cores is recommended (though not strictly required); at least 50GB of free space on a local filesystem (like /tmp ); at least 16GB of memory (2GB/core or higher recommended); Instructions to install Singularity and screen (click to show commands): CentOS 8 ( x86_64 or aarch64 or ppc64le ) sudo dnf install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-8.noarch.rpm sudo dnf update -y sudo dnf install -y screen singularity Setting up the container \u00b6 Warning It is highly recommended to start a screen or tmux session first! A container image is provided that includes everything that is required to set up a writable overlay on top of the EESSI CernVM-FS repository. First, pick a location on a local filesystem for the temporary directory: Requirements: Do not use a shared filesystem like NFS, Lustre or GPFS. There should be at least 50GB of free disk space in this local filesystem (more is better). There should be no automatic cleanup of old files via a cron job on this local filesystem. Try to make sure the directory is unique (not used by anything else). NB. If you are going to install on a separate drive (due to lack of space on /), then you need to set some variables to point to that location. You will also need to bind mount it in the singularity command. Let's say that you drive is mounted in /srt. Then you change the relevant commands below to this: export EESSI_TMPDIR = /srt/ $USER /EESSI mkdir -p $EESSI_TMPDIR mkdir /srt/tmp export SINGULARITY_BIND = \" $EESSI_TMPDIR /var-run-cvmfs:/var/run/cvmfs, $EESSI_TMPDIR /var-lib-cvmfs:/var/lib/cvmfs,/srt/tmp:/tmp\" singularity shell -B /srt --fusemount \" $EESSI_PILOT_READONLY \" --fusemount \" $EESSI_PILOT_WRITABLE_OVERLAY \" docker://ghcr.io/eessi/build-node:debian10 We will assume that /tmp/$USER/EESSI meets these requirements: export EESSI_TMPDIR = /tmp/ $USER /EESSI mkdir -p $EESSI_TMPDIR Create some subdirectories in this temporary directory: mkdir -p $EESSI_TMPDIR / { home,overlay-upper,overlay-work } mkdir -p $EESSI_TMPDIR / { var-lib-cvmfs,var-run-cvmfs } Configure Singularity cache directory, bind mounts, and (fake) home directory: export SINGULARITY_CACHEDIR = $EESSI_TMPDIR /singularity_cache export SINGULARITY_BIND = \" $EESSI_TMPDIR /var-run-cvmfs:/var/run/cvmfs, $EESSI_TMPDIR /var-lib-cvmfs:/var/lib/cvmfs\" export SINGULARITY_HOME = \" $EESSI_TMPDIR /home:/home/ $USER \" Define values to pass to --fusemount` in singularity`` command: export EESSI_PILOT_READONLY = \"container:cvmfs2 pilot.eessi-hpc.org /cvmfs_ro/pilot.eessi-hpc.org\" export EESSI_PILOT_WRITABLE_OVERLAY = \"container:fuse-overlayfs -o lowerdir=/cvmfs_ro/pilot.eessi-hpc.org -o upperdir= $EESSI_TMPDIR /overlay-upper -o workdir= $EESSI_TMPDIR /overlay-work /cvmfs/pilot.eessi-hpc.org\" Start the container (which includes Debian 10, CernVM-FS and fuse-overlayfs ): singularity shell --fusemount \" $EESSI_PILOT_READONLY \" --fusemount \" $EESSI_PILOT_WRITABLE_OVERLAY \" docker://ghcr.io/eessi/build-node:debian10 Once the container image has been downloaded and converted to a Singularity image (SIF format), you should get a prompt like this: ... CernVM-FS: loading Fuse module... done Singularity> and the EESSI CernVM-FS repository should be mounted: Singularity> ls /cvmfs/pilot.eessi-hpc.org 2020.12 2021.03 latest Setting up the environment \u00b6 Set up the environment by starting a Gentoo Prefix session using the startprefix command. Make sure you use the correct version of the EESSI pilot repository! export EESSI_PILOT_VERSION = '2021.03' /cvmfs/pilot.eessi-hpc.org/ ${ EESSI_PILOT_VERSION } /compat/linux/ $( uname -m ) /startprefix Installing software \u00b6 Clone the software-layer repository: git clone https://github.com/EESSI/software-layer.git Run the software installation script in software-layer : cd software-layer ./EESSI-pilot-install-software.sh This script will figure out the CPU microarchitecture of the host automatically (like x86_64/intel/haswell ). To build generic software installations (like x86_64/generic ), use the --generic option: ./EESSI-pilot-install-software.sh --generic Once all missing software has been installed, you should see a message like this: No missing modules! Creating tarball to ingest \u00b6 Before tearing down the build node, you should create tarball to ingest into the EESSI CernVM-FS repository. To create a tarball of all installations, assuming your build host is x86_64/intel/haswell : export EESSI_PILOT_VERSION = '2021.03' cd /cvmfs/pilot.eessi-hpc.org/ ${ EESSI_PILOT_VERSION } /software/linux eessi_tar_gz = \" $HOME /eessi- ${ EESSI_PILOT_VERSION } -haswell.tar.gz\" tar cvfz ${ eessi_tar_gz } x86_64/intel/haswell To create a tarball for specific installations, make sure you pick up both the software installation directories and the corresponding module files: eessi_tar_gz = \" $HOME /eessi- ${ EESSI_PILOT_VERSION } -haswell-OpenFOAM.tar.gz\" tar cvfz ${ eessi_tar_gz } x86_64/intel/haswell/software/OpenFOAM modules/all//OpenFOAM This tarball should be uploaded to the Stratum 0 server for ingestion. If needed, you can ask for help in the EESSI #software-layer Slack channel","title":"Build nodes"},{"location":"software_layer/build_nodes/#build-nodes","text":"Any system can be used as a build node to create additional software installations that should be added to the EESSI CernVM-FS repository.","title":"Build nodes"},{"location":"software_layer/build_nodes/#requirements","text":"OS and software: GNU/Linux (any distribution) as operating system; a recent version of Singularity (>= 3.6 is recommended); check with singularity --version screen or tmux is highly recommended; Admin privileges are not required, as long as Singularity is installed. Resources: 8 or more cores is recommended (though not strictly required); at least 50GB of free space on a local filesystem (like /tmp ); at least 16GB of memory (2GB/core or higher recommended); Instructions to install Singularity and screen (click to show commands): CentOS 8 ( x86_64 or aarch64 or ppc64le ) sudo dnf install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-8.noarch.rpm sudo dnf update -y sudo dnf install -y screen singularity","title":"Requirements"},{"location":"software_layer/build_nodes/#setting-up-the-container","text":"Warning It is highly recommended to start a screen or tmux session first! A container image is provided that includes everything that is required to set up a writable overlay on top of the EESSI CernVM-FS repository. First, pick a location on a local filesystem for the temporary directory: Requirements: Do not use a shared filesystem like NFS, Lustre or GPFS. There should be at least 50GB of free disk space in this local filesystem (more is better). There should be no automatic cleanup of old files via a cron job on this local filesystem. Try to make sure the directory is unique (not used by anything else). NB. If you are going to install on a separate drive (due to lack of space on /), then you need to set some variables to point to that location. You will also need to bind mount it in the singularity command. Let's say that you drive is mounted in /srt. Then you change the relevant commands below to this: export EESSI_TMPDIR = /srt/ $USER /EESSI mkdir -p $EESSI_TMPDIR mkdir /srt/tmp export SINGULARITY_BIND = \" $EESSI_TMPDIR /var-run-cvmfs:/var/run/cvmfs, $EESSI_TMPDIR /var-lib-cvmfs:/var/lib/cvmfs,/srt/tmp:/tmp\" singularity shell -B /srt --fusemount \" $EESSI_PILOT_READONLY \" --fusemount \" $EESSI_PILOT_WRITABLE_OVERLAY \" docker://ghcr.io/eessi/build-node:debian10 We will assume that /tmp/$USER/EESSI meets these requirements: export EESSI_TMPDIR = /tmp/ $USER /EESSI mkdir -p $EESSI_TMPDIR Create some subdirectories in this temporary directory: mkdir -p $EESSI_TMPDIR / { home,overlay-upper,overlay-work } mkdir -p $EESSI_TMPDIR / { var-lib-cvmfs,var-run-cvmfs } Configure Singularity cache directory, bind mounts, and (fake) home directory: export SINGULARITY_CACHEDIR = $EESSI_TMPDIR /singularity_cache export SINGULARITY_BIND = \" $EESSI_TMPDIR /var-run-cvmfs:/var/run/cvmfs, $EESSI_TMPDIR /var-lib-cvmfs:/var/lib/cvmfs\" export SINGULARITY_HOME = \" $EESSI_TMPDIR /home:/home/ $USER \" Define values to pass to --fusemount` in singularity`` command: export EESSI_PILOT_READONLY = \"container:cvmfs2 pilot.eessi-hpc.org /cvmfs_ro/pilot.eessi-hpc.org\" export EESSI_PILOT_WRITABLE_OVERLAY = \"container:fuse-overlayfs -o lowerdir=/cvmfs_ro/pilot.eessi-hpc.org -o upperdir= $EESSI_TMPDIR /overlay-upper -o workdir= $EESSI_TMPDIR /overlay-work /cvmfs/pilot.eessi-hpc.org\" Start the container (which includes Debian 10, CernVM-FS and fuse-overlayfs ): singularity shell --fusemount \" $EESSI_PILOT_READONLY \" --fusemount \" $EESSI_PILOT_WRITABLE_OVERLAY \" docker://ghcr.io/eessi/build-node:debian10 Once the container image has been downloaded and converted to a Singularity image (SIF format), you should get a prompt like this: ... CernVM-FS: loading Fuse module... done Singularity> and the EESSI CernVM-FS repository should be mounted: Singularity> ls /cvmfs/pilot.eessi-hpc.org 2020.12 2021.03 latest","title":"Setting up the container"},{"location":"software_layer/build_nodes/#setting-up-the-environment","text":"Set up the environment by starting a Gentoo Prefix session using the startprefix command. Make sure you use the correct version of the EESSI pilot repository! export EESSI_PILOT_VERSION = '2021.03' /cvmfs/pilot.eessi-hpc.org/ ${ EESSI_PILOT_VERSION } /compat/linux/ $( uname -m ) /startprefix","title":"Setting up the environment"},{"location":"software_layer/build_nodes/#installing-software","text":"Clone the software-layer repository: git clone https://github.com/EESSI/software-layer.git Run the software installation script in software-layer : cd software-layer ./EESSI-pilot-install-software.sh This script will figure out the CPU microarchitecture of the host automatically (like x86_64/intel/haswell ). To build generic software installations (like x86_64/generic ), use the --generic option: ./EESSI-pilot-install-software.sh --generic Once all missing software has been installed, you should see a message like this: No missing modules!","title":"Installing software"},{"location":"software_layer/build_nodes/#creating-tarball-to-ingest","text":"Before tearing down the build node, you should create tarball to ingest into the EESSI CernVM-FS repository. To create a tarball of all installations, assuming your build host is x86_64/intel/haswell : export EESSI_PILOT_VERSION = '2021.03' cd /cvmfs/pilot.eessi-hpc.org/ ${ EESSI_PILOT_VERSION } /software/linux eessi_tar_gz = \" $HOME /eessi- ${ EESSI_PILOT_VERSION } -haswell.tar.gz\" tar cvfz ${ eessi_tar_gz } x86_64/intel/haswell To create a tarball for specific installations, make sure you pick up both the software installation directories and the corresponding module files: eessi_tar_gz = \" $HOME /eessi- ${ EESSI_PILOT_VERSION } -haswell-OpenFOAM.tar.gz\" tar cvfz ${ eessi_tar_gz } x86_64/intel/haswell/software/OpenFOAM modules/all//OpenFOAM This tarball should be uploaded to the Stratum 0 server for ingestion. If needed, you can ask for help in the EESSI #software-layer Slack channel","title":"Creating tarball to ingest"}]}